{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing that generated the YAML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing required to produce the datasets include these steps:\n",
    "\n",
    "  - Obtain a short `path` for each domain:\n",
    "    - infer a path for `Domain`s from contents\n",
    "    - infer a path for `Cluster`s from contents\n",
    "  - `guid`: ensure existence of the globally unique identified (GUID) provided by the common core\n",
    "  - ensure ccssUri returns a 200\n",
    "  - meld with cleaned edge data:\n",
    "    - remove `||` comments\n",
    "    - expand `,`-  and `;`- separated lists\n",
    "    - confirm existence of `Standard`s for all edge endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-06-24 17:31:35--  http://asn.jesandco.org/resources/D10003FB_full.json\n",
      "Resolving asn.jesandco.org... 52.6.235.143\n",
      "Connecting to asn.jesandco.org|52.6.235.143|:80... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: http://s3.amazonaws.com/asnstaticd2l/data/rdf/D10003FB.json [following]\n",
      "--2016-06-24 17:31:36--  http://s3.amazonaws.com/asnstaticd2l/data/rdf/D10003FB.json\n",
      "Resolving s3.amazonaws.com... 54.231.40.178\n",
      "Connecting to s3.amazonaws.com|54.231.40.178|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1768569 (1.7M) [application/json]\n",
      "Saving to: 'D10003FB_full.json.2'\n",
      "\n",
      "D10003FB_full.json. 100%[=====================>]   1.69M   196KB/s   in 8.9s   \n",
      "\n",
      "2016-06-24 17:31:45 (193 KB/s) - 'D10003FB_full.json.2' saved [1768569/1768569]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get data from ASN\n",
    "!wget http://asn.jesandco.org/resources/D10003FB_full.json\n",
    "asn_data = json.load(open('D10003FB_full.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL typeS SEEN: [u'Statement', u'StandardDocument', 'ContainerDocument']\n",
      "ALL labelS SEEN: [u'Standard', u'Component', u'Cluster', u'Domain']\n"
     ]
    }
   ],
   "source": [
    "len(asn_data)\n",
    "\n",
    "\n",
    "def dictify_asn(asn_data):\n",
    "    SKIP_KEYS = ['authorityStatus', 'educationLevel', 'listID', 'language', 'indexingStatus']\n",
    "    results = {}\n",
    "    types_seen_so_far = []\n",
    "    labels_seen_so_far = []\n",
    "    for asnid, data in asn_data.iteritems():\n",
    "        result = {} \n",
    "        for key, values in data.iteritems():\n",
    "            newkey = key.replace('http://purl.org/ASN/schema/core/','')\n",
    "            newkey = newkey.replace('http://purl.org/dc/terms/','')\n",
    "            newkey = newkey.replace('http://www.w3.org/2004/02/skos/core#','')\n",
    "            newkey = newkey.replace('http://www.w3.org/1999/02/22-rdf-syntax-ns#','')\n",
    "            newkey = newkey.replace('http://purl.org/gem/qualifiers/','')\n",
    "            newvals = []\n",
    "            for valitem in values:\n",
    "                newvals.append(valitem['value'])\n",
    "            if newkey not in SKIP_KEYS:            # do not copy over SKIP_KEYS\n",
    "                result[newkey] = newvals\n",
    "        #\n",
    "        if result.has_key('type'):\n",
    "            result['type'] = result['type'][0].replace('http://purl.org/ASN/schema/core/','')\n",
    "        else:\n",
    "            result['type'] = 'ContainerDocument'  # add type for the main container document\n",
    "        if result.has_key('type'):\n",
    "            thetype = result['type']\n",
    "            if thetype not in types_seen_so_far:\n",
    "                types_seen_so_far.append(thetype)\n",
    "        #\n",
    "        # \n",
    "        if result.has_key('statementLabel'):\n",
    "            label = result['statementLabel'][0]\n",
    "            result['statementLabel'] = label\n",
    "            if label not in labels_seen_so_far:\n",
    "                labels_seen_so_far.append(label)\n",
    "        results[asnid] = result\n",
    "    print \"ALL typeS SEEN:\", types_seen_so_far\n",
    "    print \"ALL labelS SEEN:\", labels_seen_so_far    \n",
    "    return results\n",
    "\n",
    "asn_dictified = dictify_asn(asn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'altStatementNotation': [u'2.MD.10'],\n",
       " u'description': [u'Draw a picture graph and a bar graph (with single-unit scale) to represent a data set with up to four categories. Solve simple put-together, take-apart, and compare problems using information presented in a bar graph.'],\n",
       " u'exactMatch': [u'http://corestandards.org/Math/Content/2/MD/D/10',\n",
       "  u'urn:guid:1E9847DB7B284F2C9E412F7EBF3A7A4D'],\n",
       " u'identifier': [u'http://purl.org/ASN/resources/S1143469'],\n",
       " u'isChildOf': [u'http://asn.jesandco.org/resources/S2390250'],\n",
       " u'isPartOf': [u'http://asn.jesandco.org/resources/D10003FB'],\n",
       " u'statementLabel': u'Standard',\n",
       " u'statementNotation': [u'CCSS.Math.Content.2.MD.D.10'],\n",
       " u'subject': [u'http://purl.org/ASN/scheme/ASNTopic/math'],\n",
       " u'type': u'Statement'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asn_dictified.keys()[26]\n",
    "asn_dictified['http://asn.jesandco.org/resources/S1143469']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: add relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://asn.jesandco.org/resources/D10003FB StandardDocument has no parents\n",
      "http://asn.jesandco.org/resources/D10003FB.xml ContainerDocument has no parents\n"
     ]
    }
   ],
   "source": [
    "def add_prerequisites_and_usedfors(datadict): \n",
    "    # first pass to add ispartof and contents attributes\n",
    "    for asnid, data in datadict.iteritems():\n",
    "        data['ispartof'] = []\n",
    "        data['contents'] = []\n",
    "        if data.has_key('isChildOf'):\n",
    "            for parent in data['isChildOf']:\n",
    "                data['ispartof'].append(parent)\n",
    "        else:\n",
    "            print asnid, data['type'], 'has no parents'\n",
    "    # second pass to add contents relationships (inverse of ispartof)\n",
    "    for asnid, data in datadict.iteritems():\n",
    "        if data.has_key('ispartof'):\n",
    "            for parent_id in data['ispartof']:\n",
    "                parent = datadict[parent_id]\n",
    "                parent['contents'].append(asnid)\n",
    "        else:\n",
    "            print asnid, data['type'], 'has no parents'\n",
    "    \n",
    "                \n",
    "            \n",
    "add_prerequisites_and_usedfors(asn_dictified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: add paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def strip_cluster_to_domain(instr):\n",
    "    domain = instr\n",
    "    pre_prefixes = ['CCSS.Math.Content.', 'CCSS.Math.Practice.']\n",
    "    prefixes = ['K.', '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.']\n",
    "    suffixes = ['.A', '.B', '.C', '.D']\n",
    "    for pre_prefix in pre_prefixes:\n",
    "        if domain.startswith(pre_prefix):\n",
    "            domain = domain.replace(pre_prefix,'')\n",
    "    for prefix in prefixes:\n",
    "        if domain.startswith(prefix):\n",
    "            domain = domain.replace(prefix,'')\n",
    "    for suffix in suffixes:\n",
    "        if domain.endswith(suffix):\n",
    "            domain = domain.replace(suffix,'')\n",
    "    return domain\n",
    "\n",
    "\n",
    "def addpaths(datadict):\n",
    "    \"\"\"\n",
    "    Ensures each data item has a `path` so we we'll be able to pivot on it in next step.\n",
    "    \"\"\"\n",
    "    # \n",
    "    for asnid, data in datadict.iteritems():\n",
    "        \n",
    "        if data.has_key('statementNotation'):\n",
    "            full_path = data['statementNotation'][0]\n",
    "            if 'Content' in full_path:\n",
    "                path = full_path.replace('CCSS.Math.Content.','')\n",
    "                data['path'] = path\n",
    "            elif 'Practice' in full_path:\n",
    "                path = full_path.replace('CCSS.Math.Practice.','')\n",
    "                data['path'] = path\n",
    "            else:\n",
    "                print \"don't know what to do with\", full_path\n",
    "        else:\n",
    "            # no statementNotation so we must infer a path from contents...\n",
    "            path_infer_success = False\n",
    "            if data.has_key('statementLabel'):\n",
    "                mylabel = data['statementLabel']\n",
    "                #\n",
    "                # Handle Domains\n",
    "                if mylabel == 'Domain':   \n",
    "                    # print data['type'], asnid, 'has no statementNotation, trying to infer from contents\"\n",
    "                    current_guess = None\n",
    "                    guesses = []\n",
    "                    if data.has_key('contents'):\n",
    "                        child_ids = data['contents']\n",
    "                        for child_id in child_ids:\n",
    "                            child = datadict[child_id]\n",
    "                            #\n",
    "                            if not child.has_key('statementNotation'):\n",
    "                                continue\n",
    "                                #print child\n",
    "                                #print \"NO STATE MENT NOTATIN IN THIS CHILD\"\n",
    "                            #\n",
    "                            sn =  child['statementNotation'][0]\n",
    "                            # print sn\n",
    "                            guess = strip_cluster_to_domain(sn)\n",
    "                            guesses.append(guess)\n",
    "                            if current_guess and guess != current_guess:\n",
    "                                print \"error children infer different parent...\"\n",
    "                                print current_guess, guess\n",
    "                            current_guess = guess\n",
    "                        if current_guess:\n",
    "                            # print guesses\n",
    "                            data['path'] = current_guess\n",
    "                            data['statementLabel'] = 'Domain'\n",
    "                            path_infer_success = True\n",
    "                        else:\n",
    "                            print 'Couldnt find Deomain for', data\n",
    "                            pprint(data['contents'])\n",
    "                            # print guesses\n",
    "                    else:\n",
    "                        print \"No contents! so can't infer Domain\"\n",
    "                #\n",
    "                # Handle clusters\n",
    "                elif mylabel == 'Cluster':\n",
    "                    if data.has_key('exactMatch'):\n",
    "                        values = data['exactMatch']\n",
    "                        url = [val for val in values if val.starswith('http://corestandards.org') ][0]\n",
    "                        path = path.replace('http://corestandards.org/Math/','')\n",
    "                        path = path.replace('Practice/','')\n",
    "                        path = path.replace('Content/','')\n",
    "                        path = path.replace('/','.')\n",
    "                        data['path'] = path\n",
    "                        path_infer_success = True\n",
    "                        print \"Cluser:\", path, 'recognized from', url\n",
    "                    else:\n",
    "                        print \"Failed to recognize cluser once:\", child_path, path\n",
    "                else:\n",
    "                    print \"UNEXPECTED LABEL\", mylabel, asnid, child['statementLabel']\n",
    "            else:\n",
    "                # print asnid, \"has no statementLabel...\"\n",
    "                if asnid == 'http://asn.jesandco.org/resources/D10003FB':\n",
    "                    path = 'CCSSM'\n",
    "                    data['path'] = path\n",
    "                    data['statementLabel'] = 'Standard'\n",
    "                    path_infer_success = True\n",
    "                elif asnid == 'http://asn.jesandco.org/resources/D10003FB.xml':\n",
    "                    # will delete after\n",
    "                    path_infer_success = True                    \n",
    "                elif data.has_key('description'):\n",
    "                    desc = data['description'][0]\n",
    "                    path =  ''.join([c for c in desc if c.isupper()])\n",
    "                    # print \"Domain \", path, 'recognized from', desc\n",
    "                    data['path'] = path\n",
    "                    data['statementLabel'] = 'Domain'\n",
    "                    path_infer_success = True\n",
    "                else:\n",
    "                    print \"no statementLabel and no description\"\n",
    "\n",
    "            #\n",
    "            # DID WE SUCCESS?\n",
    "            if not path_infer_success:\n",
    "                print \"Failed to infer path\", asnid\n",
    "    #\n",
    "    # The ASN ContainerDocument has no links so safe to delete\n",
    "    if datadict.has_key('http://asn.jesandco.org/resources/D10003FB.xml'):\n",
    "        del datadict['http://asn.jesandco.org/resources/D10003FB.xml']\n",
    "\n",
    "addpaths(asn_dictified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://asn.jesandco.org/resources/S114342E']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asn_dictified.keys()[120:121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contents': [u'http://asn.jesandco.org/resources/S1143454'],\n",
       " u'description': [u'Add and subtract within 20.'],\n",
       " u'exactMatch': [u'http://corestandards.org/Math/Content/2/OA/B',\n",
       "  u'urn:guid:21FF72D85AF248E28B8AD028ABF94DDE'],\n",
       " u'hasChild': [u'http://asn.jesandco.org/resources/S1143454'],\n",
       " u'isChildOf': [u'http://asn.jesandco.org/resources/S114340A'],\n",
       " u'isPartOf': [u'http://asn.jesandco.org/resources/D10003FB'],\n",
       " 'ispartof': [u'http://asn.jesandco.org/resources/S114340A'],\n",
       " 'path': u'2.OA.B',\n",
       " u'statementLabel': u'Cluster',\n",
       " u'statementNotation': [u'CCSS.Math.Content.2.OA.B'],\n",
       " u'subject': [u'http://purl.org/ASN/scheme/ASNTopic/math'],\n",
       " u'type': u'Statement'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asn_dictified['http://asn.jesandco.org/resources/S2390246']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 704\n"
     ]
    }
   ],
   "source": [
    "# confirm paths exist and unique\n",
    "paths_seen_so_far = []\n",
    "for asnid, data in asn_dictified.iteritems():\n",
    "    #\n",
    "    # PATHs check\n",
    "    if data.has_key('path'):\n",
    "        path = data['path']\n",
    "        if path in paths_seen_so_far:\n",
    "            print \"ERROR---duplicate path\", path\n",
    "        else:\n",
    "            paths_seen_so_far.append(path)\n",
    "    else:\n",
    "        print 'NO PATH'\n",
    "        print data\n",
    "\n",
    "# report summary statistics\n",
    "print \"Number of items:\", len(paths_seen_so_far)\n",
    "\n",
    "def report_counts(asn_dictified):\n",
    "    counts = { 'standard':0, 'component':0, 'cluster':0, 'domain':0 }\n",
    "    for asnid, data in asn_dictified.iteritems():\n",
    "        if data['statementLabel']:\n",
    "            label = data['statementLabel']\n",
    "            if label == 'Standard':\n",
    "                counts['standard'] += 1\n",
    "            elif label == 'Component':\n",
    "                counts['component'] += 1\n",
    "            elif label == 'Cluster':\n",
    "                counts['cluster'] += 1\n",
    "            elif label == 'Domain':\n",
    "                counts['domain'] += 1\n",
    "            else:\n",
    "                print \"UNKNOWN LABEL!\", asnid\n",
    "        else:\n",
    "            print \"Has no label\", asnid\n",
    "    print counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster': 147, 'domain': 39, 'component': 124, 'standard': 394}\n"
     ]
    }
   ],
   "source": [
    "# print \n",
    "report_counts(asn_dictified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: normalize attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asn_normalized = asn_dictified.copy()\n",
    "for asnid, data in asn_normalized.iteritems():\n",
    "    if data.has_key('description'):\n",
    "        data['description'] = data['description'][0]\n",
    "    if data.has_key('isPartOf'):\n",
    "        del data['isPartOf']  # all identical to 'http://asn.jesandco.org/resources/D10003FB'\n",
    "    if data.has_key('identifier'):\n",
    "        data['identifier'] = data['identifier'][0]\n",
    "        data['asn_url'] = data['identifier']\n",
    "    if data.has_key('statementNotation'):\n",
    "        data['statementNotation'] = data['statementNotation'][0]\n",
    "    if data.has_key('altStatementNotation'):\n",
    "        data['alias'] = data['altStatementNotation'][0]\n",
    "    if data.has_key('subject'):\n",
    "        del data['subject']  # we kind of know it's math ;)\n",
    "    if data.has_key('exactMatch'):\n",
    "        for fk in data['exactMatch']:\n",
    "            if fk.startswith('http://corestandards.org'):\n",
    "                data['ccss_url'] = fk\n",
    "            elif fk.startswith('urn:guid:'):\n",
    "                data['ccss_guid'] = fk.replace('urn:guid:','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster': 147, 'domain': 39, 'component': 124, 'standard': 394}\n",
      "{'cluster': 147, 'domain': 39, 'component': 124, 'standard': 394}\n"
     ]
    }
   ],
   "source": [
    "report_counts(asn_dictified)\n",
    "report_counts(asn_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and process edge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "edges_raw = csv.DictReader( open('GraphData-EdgesOnly-Grade-00-08-2011-06-01-cleaned - EdgeSet.csv').readlines() )\n",
    "edges_raw = list(edges_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove expand commas and semicolons which represent multiple nodes\n",
    "# convert 0. to K.\n",
    "\n",
    "\n",
    "def csv_to_list(instr):\n",
    "    if ',' in instr:\n",
    "        paths = []\n",
    "        csvals = instr.split(',')\n",
    "        basenode = csvals[0]\n",
    "        paths.append(basenode)\n",
    "        for i in range(1,len(csvals)):\n",
    "             paths.append(basenode[:-1] + csvals[i])\n",
    "        return paths\n",
    "    else:\n",
    "        return [instr]\n",
    "\n",
    "                \n",
    "def split_comments(instr):\n",
    "    \"\"\"Processes special formatting ( || and , and ;) in edge endpoint spec.\"\"\"\n",
    "    nodes = []\n",
    "    comment = None\n",
    "    \n",
    "    if '||' in instr:\n",
    "        nodedata, comment = instr.split('||')\n",
    "    else:\n",
    "        nodedata = instr\n",
    "        comment = None\n",
    "\n",
    "    if ';' in nodedata:\n",
    "        tmpnodes = nodedata.split(';')\n",
    "        # for ,-separated list within a ;-separated list\n",
    "        nodes = []\n",
    "        for node in tmpnodes:\n",
    "            nodes.extend(csv_to_list(node))\n",
    "    elif ',' in nodedata:\n",
    "        nodes = csv_to_list(nodedata)\n",
    "    else:\n",
    "        nodes = [nodedata]\n",
    "    #\n",
    "    # rename 0. to K. \n",
    "    knodes = []\n",
    "    for node in nodes:\n",
    "        if node.startswith('0.'):\n",
    "            node = node.replace('0.','K.')\n",
    "            knodes.append(node)\n",
    "        else:\n",
    "            knodes.append(node)\n",
    "    # manually fix domain links 'K.OA', 1.OA' 2.OA' \n",
    "    koanodes = []\n",
    "    for node in knodes:\n",
    "        if node in ['K.OA', '1.OA', '2.OA']:\n",
    "            node = 'OA'\n",
    "        koanodes.append(node)\n",
    "    #\n",
    "    return koanodes, comment\n",
    "\n",
    "\n",
    "edges = []\n",
    "for raw_edge in edges_raw:\n",
    "    if raw_edge['Note']:\n",
    "        print \"NOTE::\", Note\n",
    "    # start\n",
    "    start_nodes, start_comment = split_comments( raw_edge['Begin'] )\n",
    "    end_nodes, end_comment = split_comments( raw_edge['End'] )\n",
    "\n",
    "    # print start_nodes, end_nodes\n",
    "    if raw_edge['EdgeDesc'] == 'Arrow':\n",
    "        for element in itertools.product(start_nodes, end_nodes):\n",
    "            edge = {'start':element[0], \n",
    "                    'start_comment': start_comment,\n",
    "                    'end':element[1],\n",
    "                    'end_comment': end_comment,\n",
    "                    'type':'usedfor'}\n",
    "            edges.append(edge)\n",
    "    elif raw_edge['EdgeDesc'] == 'Nondirectional link':\n",
    "        for element in itertools.product(start_nodes, end_nodes):\n",
    "            edge = {'start':element[0], \n",
    "                    'start_comment': start_comment,\n",
    "                    'end':element[1],\n",
    "                    'end_comment': end_comment,\n",
    "                    'type':'related'}\n",
    "            edges.append(edge)\n",
    "    else:\n",
    "        print \"UKNOWN EDGE TYPE\"\n",
    "\n",
    "        \n",
    "# clean edges and  remove duplicates and self-edges\n",
    "from collections import defaultdict\n",
    "edges_dict = defaultdict(list)\n",
    "for edge in edges:\n",
    "    # use tuple (start,end) as keys\n",
    "    edges_dict[ (edge['start'],edge['end']) ].append(edge)\n",
    "#\n",
    "cleaned_edges = []\n",
    "for endpoints, edgelist in edges_dict.iteritems():\n",
    "    edge = edgelist[0]\n",
    "    if edge['start'] == edge['end']:\n",
    "        continue\n",
    "    cleaned_edges.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 567, 542)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges), len(edges_dict), len(cleaned_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using path for OA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# utility function\n",
    "def find_by_path_or_alias(path):\n",
    "    \"\"\"\n",
    "    Looks through `asn_dictified` and retunrs the asnid of element with this path.\n",
    "    \"\"\"\n",
    "    # print \"looking for\", path\n",
    "    found = False\n",
    "    result = None\n",
    "    for asnid, data in asn_normalized.iteritems():\n",
    "        if data.has_key('alias'):\n",
    "            if path == data['alias']:\n",
    "                # print 'using alias'\n",
    "                return asnid\n",
    "        elif data.has_key('path'):\n",
    "            if path == data['path']:\n",
    "                print \"using path for\", path\n",
    "                return asnid\n",
    "        else:\n",
    "            print \"No alias or path\"\n",
    "    if not found:\n",
    "        print \"not found\", path\n",
    "\n",
    "\n",
    "# ensure we can find both edges of relationship\n",
    "for edge in cleaned_edges:\n",
    "    ansstart = find_by_path_or_alias(edge['start'])\n",
    "    asnend = find_by_path_or_alias(edge['end'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6: add prerequistite and related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using path for OA\n"
     ]
    }
   ],
   "source": [
    "def create_or_append_to_list_attribute(thedict, attr_name, value):\n",
    "    if thedict.has_key(attr_name):\n",
    "        thedict[attr_name].append(value)\n",
    "    else:\n",
    "        thedict[attr_name]=[value]\n",
    "    \n",
    "\n",
    "for edge in cleaned_edges:\n",
    "    start_id = find_by_path_or_alias(edge['start'])\n",
    "    start = asn_normalized[start_id]\n",
    "    end_id = find_by_path_or_alias(edge['end'])\n",
    "    end = asn_normalized[end_id]\n",
    "    \n",
    "    if edge['type'] == 'usedfor':\n",
    "        create_or_append_to_list_attribute(start, 'usedfors',      end_id)\n",
    "        create_or_append_to_list_attribute(end,   'prerequisites', start_id)\n",
    "        \n",
    "    elif edge['type'] == 'related':\n",
    "        create_or_append_to_list_attribute(start, 'related', end_id)\n",
    "        create_or_append_to_list_attribute(end,   'related', start_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 7: replace all FK with `path`s instead of asnids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_dict = {}  # pivot asn_normalized to use path as keys, and foreign keys\n",
    "\n",
    "for asnid, data in asn_normalized.items():\n",
    "    clone = data.copy()\n",
    "    fk_fields = ['contents', 'ispartof', 'related', 'prerequisites', 'usedfors']\n",
    "    for field in fk_fields:\n",
    "        if data.has_key(field):\n",
    "            asnids_list = data[field]\n",
    "            clone[field]=[asn_normalized[asnid]['path'] for asnid in asnids_list]\n",
    "    #\n",
    "    path_dict[data['path']]=clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster': 147, 'domain': 39, 'component': 124, 'standard': 394}\n"
     ]
    }
   ],
   "source": [
    "report_counts(path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asn_url': u'http://purl.org/ASN/resources/S114340A',\n",
       " 'contents': [u'K.OA.A',\n",
       "  u'3.OA.C',\n",
       "  u'3.OA.B',\n",
       "  u'2.OA.B',\n",
       "  u'2.OA.A',\n",
       "  u'1.OA.C',\n",
       "  u'1.OA.A',\n",
       "  u'1.OA.B',\n",
       "  u'3.OA.A',\n",
       "  u'3.OA.D',\n",
       "  u'1.OA.D',\n",
       "  u'5.OA.A',\n",
       "  u'5.OA.B',\n",
       "  u'2.OA.C',\n",
       "  u'4.OA.C',\n",
       "  u'4.OA.B',\n",
       "  u'4.OA.A'],\n",
       " u'description': u'Operations and Algebraic Thinking',\n",
       " u'hasChild': [u'http://asn.jesandco.org/resources/S1143411',\n",
       "  u'http://asn.jesandco.org/resources/S114342D',\n",
       "  u'http://asn.jesandco.org/resources/S2390245',\n",
       "  u'http://asn.jesandco.org/resources/S114342E',\n",
       "  u'http://asn.jesandco.org/resources/S114342F',\n",
       "  u'http://asn.jesandco.org/resources/S2390246',\n",
       "  u'http://asn.jesandco.org/resources/S1143430',\n",
       "  u'http://asn.jesandco.org/resources/S114344F',\n",
       "  u'http://asn.jesandco.org/resources/S114346D',\n",
       "  u'http://asn.jesandco.org/resources/S114346E',\n",
       "  u'http://asn.jesandco.org/resources/S114346F',\n",
       "  u'http://asn.jesandco.org/resources/S1143470',\n",
       "  u'http://asn.jesandco.org/resources/S1143490',\n",
       "  u'http://asn.jesandco.org/resources/S1143491',\n",
       "  u'http://asn.jesandco.org/resources/S1143492',\n",
       "  u'http://asn.jesandco.org/resources/S11434B6',\n",
       "  u'http://asn.jesandco.org/resources/S11434B7'],\n",
       " u'identifier': u'http://purl.org/ASN/resources/S114340A',\n",
       " u'isChildOf': [u'http://asn.jesandco.org/resources/D10003FB'],\n",
       " 'ispartof': ['CCSSM'],\n",
       " 'path': u'OA',\n",
       " u'statementLabel': 'Domain',\n",
       " u'type': u'Statement',\n",
       " 'usedfors': [u'5.OA.A.2']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dict['OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from alias to aliases and other normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_path_dict = {}\n",
    "for item_key, item in path_dict.items():\n",
    "    item = item.copy()\n",
    "    if item.has_key('alias'):\n",
    "        if item['alias'] == item['path']:\n",
    "            pass\n",
    "        else:\n",
    "            item['aliases'] = [item['alias']]\n",
    "        del item['alias']\n",
    "    if item.has_key('comment'):\n",
    "        item['comment'] = '\\n'.join( item['comment'] )\n",
    "    if item.has_key('contents') and len(item['contents'])==0:\n",
    "        del item['contents']\n",
    "    if item.has_key('ispartof') and len(item['ispartof'])==0:\n",
    "        del item['ispartof']\n",
    "    item['__class__'] = item['statementLabel']\n",
    "    del item['statementLabel']\n",
    "    normalized_path_dict[item_key] = item\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: write out YAML files to ccssm/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from collections import OrderedDict\n",
    "DATA_DIR = 'ccssm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_attributes = ['path',\n",
    "                     '__class__',\n",
    "                     'aliases',\n",
    "                     'description',\n",
    "                     'comment',\n",
    "                     'contents',\n",
    "                     'ispartof',\n",
    "                     'prerequisites',\n",
    "                     'usedfors',\n",
    "                     'related',\n",
    "                     'ccss_guid',\n",
    "                     'ccss_url',\n",
    "                     'asn_url']\n",
    "export_list = []\n",
    "for path, data in normalized_path_dict.items():\n",
    "    clone = OrderedDict()\n",
    "    for attr in export_attributes:\n",
    "        if data.has_key(attr):\n",
    "            clone[attr]=data[attr]\n",
    "    export_list.append(clone)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.': 35,\n",
      " '2.': 38,\n",
      " '3.': 48,\n",
      " '4.': 49,\n",
      " '5.': 51,\n",
      " '6.': 57,\n",
      " '7.': 52,\n",
      " '8.': 46,\n",
      " 'HSA': 50,\n",
      " 'HSF': 60,\n",
      " 'HSG': 67,\n",
      " 'HSN': 46,\n",
      " 'HSS': 50,\n",
      " 'K.': 34,\n",
      " 'domain': 12,\n",
      " 'practice': 8,\n",
      " 'standard': 1,\n",
      " 'unrecognized': 0}\n",
      "total 704\n"
     ]
    }
   ],
   "source": [
    "# analytics\n",
    "\n",
    "def counts_by_grade(export_list):\n",
    "    grades_counts = {  \n",
    "       'K.':0,\n",
    "       '1.':0,\n",
    "       '2.':0,\n",
    "       '3.':0,\n",
    "       '4.':0,\n",
    "       '5.':0,\n",
    "       '6.':0,\n",
    "       '7.':0,\n",
    "       '8.':0,\n",
    "       'HSA':0,\n",
    "       'HSF':0,\n",
    "       'HSG':0,\n",
    "       'HSN':0,\n",
    "       'HSS':0,\n",
    "    }\n",
    "    other_counts = {\n",
    "       'practice':0,\n",
    "       'domain':0,\n",
    "       'standard':0,        \n",
    "       'unrecognized':0,        \n",
    "    }\n",
    "    unrecognized_list = []\n",
    "    for item in export_list:\n",
    "        grade_recognized = False\n",
    "        for key in grades_counts.keys():\n",
    "            if item['path'].startswith(key):\n",
    "                grades_counts[key] +=1\n",
    "                grade_recognized = True\n",
    "        if not grade_recognized:\n",
    "            if item['path'].startswith('MP'):\n",
    "                other_counts['practice'] += 1            \n",
    "            elif item.get('__class__',None) == 'Domain':\n",
    "                other_counts['domain'] += 1\n",
    "            elif item.get('__class__',None) == 'Standard':\n",
    "                other_counts['standard'] += 1\n",
    "            else:\n",
    "                other_counts['unrecognized'] += 1\n",
    "                unrecognized_list.append(item)\n",
    "    \n",
    "    if len(unrecognized_list) > 0:\n",
    "        print \"Unrecognized nodes! \", unrecognized_list\n",
    "    all_counts = grades_counts.copy()\n",
    "    all_counts.update(other_counts)\n",
    "    pprint( all_counts )\n",
    "    print 'total', sum( all_counts.values() )\n",
    "    return unrecognized_list\n",
    "\n",
    "\n",
    "unrec_list = counts_by_grade(export_list)\n",
    "for item in unrec_list:\n",
    "    print item['path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML writer helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import yaml\n",
    "\n",
    "\n",
    "# custom representer to handle OrderedDict\n",
    "def represent_ordereddict(dumper, data):\n",
    "    value = []\n",
    "    for item_key, item_value in data.items():\n",
    "        node_key = dumper.represent_data(item_key)\n",
    "        if node_key.value == 'description':\n",
    "            node_value = dumper.represent_scalar(u'tag:yaml.org,2002:str', item_value, style='>')        \n",
    "        else:\n",
    "            node_value = dumper.represent_data(item_value)\n",
    "        # print node_value\n",
    "        value.append((node_key, node_value))\n",
    "    return yaml.nodes.MappingNode(u'tag:yaml.org,2002:map', value)\n",
    "yaml.SafeDumper.add_representer(OrderedDict, represent_ordereddict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_list_to_file(export_list, filename):\n",
    "    #\n",
    "    #\n",
    "    dense_yaml = yaml.safe_dump(export_list,\n",
    "                                allow_unicode=True, \n",
    "                                default_flow_style=False, \n",
    "                                indent=2)\n",
    "    sparse_yaml_lines = []\n",
    "    for line in dense_yaml.splitlines():\n",
    "        line = line.replace('  description: >-',\n",
    "                            '  description: >')\n",
    "        line = line.replace('- path:', \n",
    "                            \"-\\n  path:\")\n",
    "        line = line.replace('  -', \n",
    "                            '    -')\n",
    "        sparse_yaml_lines.append(line)\n",
    "\n",
    "\n",
    "    dataout = '\\n'.join(sparse_yaml_lines)\n",
    "    unicode_dataout = unicode(dataout,'utf-8')\n",
    "    outfile = codecs.open(filename,'w', encoding='utf-8')\n",
    "    outfile.write(unicode_dataout)\n",
    "    print \"wrote\", len(export_list), \"items to\", filename\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Sort in alphabetically \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Order alphabetically\n",
    "\n",
    "def alpha_but_kfirst_sort_function(path):\n",
    "    if path.startswith('K.'):\n",
    "        path = path.replace('K.','0.')\n",
    "        return path\n",
    "    else:\n",
    "        return path\n",
    "\n",
    "# sort whole list\n",
    "sorted_export_list = sorted(export_list, key=lambda v: alpha_but_kfirst_sort_function(v['path']))\n",
    "\n",
    "# sort fk-like attributes\n",
    "for item in sorted_export_list:\n",
    "    attrs_to_sort = ['prerequsities', 'usedfors', 'related', 'contents', 'ispartof']\n",
    "    for attr in attrs_to_sort:\n",
    "        if item.has_key(attr):\n",
    "            item[attr] = sorted(item[attr], key=alpha_but_kfirst_sort_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping all.yml export step\n"
     ]
    }
   ],
   "source": [
    "# EXPORT ALL IN ONE SHOT\n",
    "#\n",
    "# outfile_name = DATA_DIR + 'all.yml'\n",
    "# write_list_to_file(sorted_export_list, outfile_name)\n",
    "\n",
    "print \"skipping all.yml export step\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by grade level and __class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_by_grade_and_class(export_list):\n",
    "    grades_items = {  \n",
    "       'K.':[],\n",
    "       '1.':[],\n",
    "       '2.':[],\n",
    "       '3.':[],\n",
    "       '4.':[],\n",
    "       '5.':[],\n",
    "       '6.':[],\n",
    "       '7.':[],\n",
    "       '8.':[],\n",
    "       'HSA':[],\n",
    "       'HSF':[],\n",
    "       'HSG':[],\n",
    "       'HSN':[],\n",
    "       'HSS':[],\n",
    "    }\n",
    "    other_items = {\n",
    "       'practice':[],\n",
    "       'domain':[],\n",
    "       'standard':[],        \n",
    "       'unrecognized':[],        \n",
    "    }\n",
    "    for item in export_list:\n",
    "        grade_recognized = False\n",
    "        for key in grades_items.keys():\n",
    "            if item['path'].startswith(key):\n",
    "                grades_items[key].append(item)\n",
    "                grade_recognized = True\n",
    "        if not grade_recognized:\n",
    "            if item['path'].startswith('MP'):\n",
    "                other_items['practice'].append(item)       \n",
    "            elif item.get('__class__',None) == 'Domain':\n",
    "                other_items['domain'].append(item)\n",
    "            elif item.get('__class__',None) == 'Standard':\n",
    "                other_items['standard'].append(item)\n",
    "            else:\n",
    "                other_items['unrecognized'].append(item)\n",
    "                unrecognized_list.append(item)\n",
    "    if len(other_items['unrecognized']) > 0:\n",
    "        print \"Unrecognized nodes! \", other_items['unrecognized']\n",
    "    else:\n",
    "        del other_items['unrecognized']\n",
    "    all_items = grades_items.copy()\n",
    "    all_items.update(other_items)\n",
    "    print 'total', sum( map(lambda v: len(v), all_items.values()) )\n",
    "    return all_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 704\n"
     ]
    }
   ],
   "source": [
    "export_lists = split_by_grade_and_class(sorted_export_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export_lists['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 12 items to ccssm/domain.yml\n",
      "wrote 50 items to ccssm/HSA.yml\n",
      "wrote 46 items to ccssm/8.yml\n",
      "wrote 67 items to ccssm/HSG.yml\n",
      "wrote 52 items to ccssm/7.yml\n",
      "wrote 8 items to ccssm/practice.yml\n",
      "wrote 46 items to ccssm/HSN.yml\n",
      "wrote 34 items to ccssm/K.yml\n",
      "wrote 57 items to ccssm/6.yml\n",
      "wrote 1 items to ccssm/standard.yml\n",
      "wrote 49 items to ccssm/4.yml\n",
      "wrote 50 items to ccssm/HSS.yml\n",
      "wrote 51 items to ccssm/5.yml\n",
      "wrote 48 items to ccssm/3.yml\n",
      "wrote 60 items to ccssm/HSF.yml\n",
      "wrote 38 items to ccssm/2.yml\n",
      "wrote 35 items to ccssm/1.yml\n"
     ]
    }
   ],
   "source": [
    "# EXPORT TO INDIVIDUAL FILES\n",
    "for list_name, list_items in export_lists.items():\n",
    "    if list_name.endswith('.'):\n",
    "        list_name = list_name[:-1]\n",
    "    #\n",
    "    outfile_name = DATA_DIR + list_name + '.yml'\n",
    "    write_list_to_file(list_items, outfile_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm we can load data back in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "ccssm_data = None\n",
    "grade5 = DATA_DIR + '5.yml'\n",
    "with open(grade5, 'r') as yaml_file:\n",
    "    ccssm_data = yaml.load(yaml_file)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__class__': 'Component',\n",
      " 'aliases': ['5.NF.5.b'],\n",
      " 'ccss_guid': 'FD677276B89E4F55AEEC482260D345C2',\n",
      " 'ccss_url': 'http://corestandards.org/Math/Content/5/NF/B/5/b',\n",
      " 'description': u'Explaining why multiplying a given number by a fraction greater than 1 results in a product greater than the given number (recognizing multiplication by whole numbers greater than 1 as a familiar case); explaining why multiplying a given number by a fraction less than 1 results in a product smaller than the given number; and relating the principle of fraction equivalence a/b = (n\\xd7a)/(n\\xd7b) to the effect of multiplying a/b by 1.\\n',\n",
      " 'ispartof': ['5.NF.B.5'],\n",
      " 'path': '5.NF.B.5b'}\n"
     ]
    }
   ],
   "source": [
    "for data in ccssm_data:\n",
    "    if data['path'] == '5.NF.B.5b':\n",
    "        pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Round trip load test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
